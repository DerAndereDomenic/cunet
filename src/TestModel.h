#pragma once

#include "../lib/include/CUNET.h"

namespace cunet
{
    class TestModel
    {
        public:
        TestModel()
        {
           float weight0[] = {0.28763535618782043, 0.2259429395198822, -0.2016950398683548, -0.226682648062706, 0.12436094880104065, 0.20048919320106506, -0.041118621826171875, -0.29954612255096436, -0.18405058979988098, 0.17937779426574707, -0.21713493764400482, -0.03562626242637634, -0.19613178074359894, 0.2915861904621124, 0.15958580374717712, 0.16831347346305847, -0.2440798282623291, -0.11509336531162262, 0.22232499718666077, -0.14505848288536072, 0.16417676210403442, 0.18821921944618225, -0.1688346564769745, 0.012199074029922485, 0.11901924014091492, 0.19402220845222473, 0.03529509902000427, 0.06416171789169312, 0.12102559208869934, -0.24185922741889954, 0.25949957966804504, -0.0619657039642334, -0.23260420560836792, 0.20664814114570618, -0.27642571926116943, 0.21161773800849915, 0.2994379699230194, 0.276212602853775, -0.28437045216560364, 0.06360286474227905, -0.11378613114356995, 0.2559961974620819, -0.1806831657886505, -0.22733905911445618, 0.04454907774925232, 0.19938716292381287, -0.2453516721725464, 0.1371431052684784, -0.0022803843021392822, 0.2377716600894928, };
           float bias0[] = {0.16374251246452332, -0.1768885999917984, -0.0002790093421936035, 0.11519140005111694, 0.1747284233570099, };
           linear0 = Linear<float>(10, 5, weight0, bias0);
           relu1 = ReLU<float>();
           float weight2[] = {0.42954307794570923, 0.39796602725982666, 0.4222725033760071, -0.15879970788955688, -0.3159794509410858, -0.19688016176223755, 0.24477779865264893, 0.11985254287719727, 0.22214341163635254, 0.11614620685577393, 0.3078334331512451, 0.07099407911300659, -0.004404276609420776, -0.268930584192276, -0.3826388120651245, -0.43110084533691406, -0.1720329225063324, 0.04314473271369934, 0.2543196678161621, 0.33826136589050293, -0.12001636624336243, 0.03343495726585388, 0.1341170072555542, -0.11130326986312866, 0.30728358030319214, };
           float bias2[] = {-0.09440481662750244, 0.172255277633667, 0.29489606618881226, 0.28442150354385376, -0.341999351978302, };
           linear2 = Linear<float>(5, 5, weight2, bias2);
           sigmoid3 = Sigmoid<float>();
           float weight4[] = {-0.22239981591701508, -0.320659339427948, 0.34074515104293823, 0.4264882206916809, -0.14181384444236755, 0.09412407875061035, 0.4274570345878601, 0.442083477973938, -0.2909933924674988, -0.20438285171985626, 0.3808862566947937, -0.15768373012542725, -0.28966224193573, -0.31637445092201233, 0.334044873714447, };
           float bias4[] = {-0.22180452942848206, 0.3337029218673706, 0.22163081169128418, };
           linear4 = Linear<float>(5, 3, weight4, bias4);
           tanh5 = Tanh<float>();
           float weight6[] = {-0.48321712017059326, -0.06612730026245117, 0.26839953660964966, 0.12456893920898438, -0.43342453241348267, 0.187696635723114, 0.059297025203704834, -0.3382832407951355, -0.10759931802749634, };
           float bias6[] = {0.06038403511047363, -0.10009542107582092, 0.3684716820716858, };
           linear6 = Linear<float>(3, 3, weight6, bias6);
           softplus7 = Softplus<float>();
        }

        __device__
        Tensor<float>& operator()(Tensor<float>& x)
        {
            x = linear0(x);
            x = relu1(x);
            x = linear2(x);
            x = sigmoid3(x);
            x = linear4(x);
            x = tanh5(x);
            x = linear6(x);
            x = softplus7(x);
            return x;
        }

        private:
        Linear<float> linear0;
        ReLU<float> relu1;
        Linear<float> linear2;
        Sigmoid<float> sigmoid3;
        Linear<float> linear4;
        Tanh<float> tanh5;
        Linear<float> linear6;
        Softplus<float> softplus7;
    }; // class
} // namespace cunet